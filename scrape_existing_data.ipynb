{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import html2text\n",
    "import logging\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tagger = BeautifulSoup('', 'html.parser')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_protozoa_links():\n",
    "    soup = BeautifulSoup(requests.get('http://www.sanger.ac.uk/resources/downloads/protozoa/').text, \"html.parser\")\n",
    "    links = [(a.text, 'http://www.sanger.ac.uk' + a['href']) for a in soup.find_all('a') if '/resources/downloads/protozoa/' in a['href']]\n",
    "    return links\n",
    "\n",
    "def get_vectors_links():\n",
    "    soup = BeautifulSoup(requests.get('http://www.sanger.ac.uk/resources/downloads/vectors/').text, \"html.parser\")\n",
    "    links = [(a.text, 'http://www.sanger.ac.uk' + a['href']) for a in soup.find_all('a') if '/resources/downloads/vectors/' in a['href']]\n",
    "    return links\n",
    "\n",
    "def get_prokaryotes_links():\n",
    "    soup = BeautifulSoup(requests.get('http://www.sanger.ac.uk/resources/downloads/bacteria/').text, \"html.parser\")\n",
    "    links = []\n",
    "    for col_class in ['col'+str(i) for i in range(1,4)]:\n",
    "        col = soup.find(class_=col_class)\n",
    "        links += [(a.text, 'http://www.sanger.ac.uk/resources/downloads/bacteria/' + a['href']) for a in col.find_all('a')]\n",
    "    return links\n",
    "\n",
    "def get_helminth_links():\n",
    "    soup = BeautifulSoup(requests.get('http://www.sanger.ac.uk/resources/downloads/helminths/').text, \"html.parser\")\n",
    "    links = [(a.text, 'http://www.sanger.ac.uk/resources/downloads/helminths/' + a['href']) for a in soup.find_all('ul')[2].find_all('a')]\n",
    "    return links\n",
    "\n",
    "def get_virus_links():\n",
    "    soup = BeautifulSoup(requests.get('http://www.sanger.ac.uk/resources/downloads/viruses/').text, \"html.parser\")\n",
    "    links = [(a.text, 'http://www.sanger.ac.uk/resources/downloads/viruses/' + a['href']) for a in soup.find_all('ul')[2].find_all('a')]\n",
    "    return links\n",
    "\n",
    "def include_ajax(soup):\n",
    "    for ajax_div in soup.find_all(class_='ajax'):\n",
    "        url = \"http://www.sanger.ac.uk\" + ajax_div['title']\n",
    "        logger.debug(\"  adding content from %s\" % url)\n",
    "        ajax_html = requests.get(url).text\n",
    "        ajax_soup = BeautifulSoup(ajax_html, 'html.parser')\n",
    "        content = ajax_soup.find(id='main-content')\n",
    "        ajax_div.replace_with(content)\n",
    "        \n",
    "def _previous_non_whitespace(start_element):\n",
    "    previous = start_element.previous_sibling\n",
    "    for i in range(20):\n",
    "        if previous is None:\n",
    "            break\n",
    "        elif not previous.name is None:\n",
    "            break\n",
    "        elif str(previous).strip() != '':\n",
    "            break\n",
    "        else:\n",
    "            previous = previous.previous_sibling\n",
    "    else:\n",
    "        logger.error(\"Something's gone wrong, we shouldn't have looped this often!\")\n",
    "        logger.error(\"previous: '%s' (%s)\" % (previous, type(previous)))\n",
    "        logger.error(\"start_element: '%s' (%s)\" % (start_element, type(start_element)))\n",
    "        raise ValueError(\"Ahhh! FIXME\")\n",
    "    return previous\n",
    "\n",
    "def _next_non_whitespace(start_element):\n",
    "    next_element = start_element.next_sibling\n",
    "    for i in range(20):\n",
    "        if next_element is None:\n",
    "            break\n",
    "        elif not next_element.name is None:\n",
    "            break\n",
    "        elif str(next_element).strip() != '':\n",
    "            break\n",
    "        else:\n",
    "            next_element = next_element.next_sibling\n",
    "    else:\n",
    "        logger.error(\"Something's gone wrong, we shouldn't have looped this often!\")\n",
    "        logger.error(\"next_element: '%s' (%s)\" % (next_element, type(next_element)))\n",
    "        logger.error(\"start_element: '%s' (%s)\" % (start_element, type(start_element)))\n",
    "        raise ValueError(\"Ahhh! FIXME\")\n",
    "    return next_element\n",
    "        \n",
    "def _remove_title(description_panel):\n",
    "    logger.debug(\"  removing species title\")\n",
    "    title_heading = description_panel.find('h2') or description_panel.find('h3')\n",
    "    title = title_heading.text\n",
    "    title_heading.replace_with(\"\")\n",
    "    return title\n",
    "\n",
    "def _remove_bibliography(description_panel):\n",
    "    bibliography_divs = [b for b in description_panel.find_all('h4') if b.text=='Bibliography']\n",
    "    bibliography_divs += [b for b in description_panel.find_all('h5') if b.text=='Bibliography']\n",
    "    bibliography_divs += [b for b in description_panel.find_all('h4') if b.text=='References']\n",
    "    if len(bibliography_divs) == 0:\n",
    "        logger.debug(\"  no bibliography to remove\")\n",
    "        return\n",
    "    (b,) = bibliography_divs\n",
    "    next_element = _next_non_whitespace(b)\n",
    "    if next_element is None:\n",
    "        logger.warning(\"  found a bibliography title but no bibliography, deleting it anyway\")\n",
    "        b.replace_with('')\n",
    "    elif next_element.name == 'ul' and 'references' in next_element[\"class\"]:\n",
    "        logger.debug(\"  removing bibliography\")\n",
    "        b.replace_with('')\n",
    "        next_element.replace_with('')\n",
    "    else:\n",
    "        logger.error(\"  can't find references adjacent to bibliography\")\n",
    "        logger.error(\"b: %s\" % b)\n",
    "        logger.error(\"next_element: %s\" % next_element)\n",
    "\n",
    "def _remove_studies(description_panel):\n",
    "    logger.debug(\"  removing study details\")\n",
    "    study_divs = [b for b in description_panel.find_all('h4') if b.text.strip()=='Studies']\n",
    "    if len(study_divs) != 0:\n",
    "        (s,) = study_divs\n",
    "        s.replace_with('')\n",
    "    sub_nav_divs = [d for d in description_panel.find_all(class_='sub_nav') if d.name == 'div']\n",
    "    if len(sub_nav_divs) != 0:\n",
    "        (d,) = sub_nav_divs\n",
    "        d.replace_with('')\n",
    "    sub_data_divs = [d for d in description_panel.find_all(class_='sub_data') if d.name == 'div']\n",
    "    if len(sub_data_divs) != 0:\n",
    "        (d,) = sub_data_divs\n",
    "        for div in d.find_all('div'):\n",
    "            if \"project_\" in div.get('id', ''):\n",
    "                div.replace_with('')\n",
    "\n",
    "def _remove_breaks_in_tables(soup):\n",
    "    logger.debug(\"  removing breaks from within tables\")\n",
    "    for table in soup.find_all('table'):\n",
    "        for br in table.find_all('br'):\n",
    "            first, *others = br.contents\n",
    "            br.replace_with(first)\n",
    "            previous = first\n",
    "            for other in others:\n",
    "                previous.insert_after(other)\n",
    "                previous = other\n",
    "\n",
    "def _add_break_before_tables(soup):\n",
    "    logger.debug(\"  adding a break before tabels\")\n",
    "    for table in soup.find_all('table'):\n",
    "        br = soup.new_tag('br')\n",
    "        table.insert_before(br)\n",
    "        \n",
    "def _remove_spans_in_tables(soup):\n",
    "    logger.debug(\"  removing spans from within tables\")\n",
    "    for table in soup.find_all('table'):\n",
    "        for span in table.find_all('span'):\n",
    "            first, *others = span.contents\n",
    "            span.replace_with(first)\n",
    "            previous = first\n",
    "            for other in others:\n",
    "                previous.insert_after(other)\n",
    "                previous = other\n",
    "\n",
    "def _add_space_between_links(soup):\n",
    "    logger.debug(\"  adding a space between links\")\n",
    "    for a in soup.find_all('a'):\n",
    "        if a.next_sibling and a.next_sibling.name == 'a':\n",
    "            space = soup.new_tag('span')\n",
    "            space.string = ' '\n",
    "            a.insert_after(space)\n",
    "            \n",
    "def _remove_rhs(description_panel):\n",
    "    logger.debug(\"  removing the panel on the rhs\")\n",
    "    rhs = description_panel.find(id='rhs')\n",
    "    if rhs:\n",
    "        rhs.replace_with('')\n",
    "     \n",
    "def _get_pubmed_id(periodical_link):\n",
    "    \n",
    "    # periodical_link.contents looks like:\n",
    "    # ['PUBMED: ', <a href=\"http://ukpmc.ac.uk/abstract/MED/15875012\">15875012</a>,\n",
    "    #  '; PMC: ', <a href=\"http://ukpmc.ac.uk/articles/PMC1352341\">1352341</a>,\n",
    "    #  '; DOI: ', <a href=\"http://dx.doi.org/10.1038%2Fnature03481\">10.1038/nature03481</a>]\n",
    "    link_bits = (bit for bit in periodical_link.contents)\n",
    "    pairs_of_link_bits = zip(link_bits, link_bits)\n",
    "    (a,) = [a for prefix,a in pairs_of_link_bits if prefix == 'PUBMED: ']\n",
    "    return a.text\n",
    "\n",
    "def _get_pubmed_ids(soup):\n",
    "    logger.debug(\"  getting pubmed ids\")\n",
    "    periodicals = soup.find_all(class_='periodical')\n",
    "    _flatten = lambda parent_list: [el for child_list in parent_list for el in child_list]\n",
    "    pub_links = _flatten([p.find_all(class_='links') for p in periodicals])\n",
    "    return [_get_pubmed_id(l) for l in pub_links]\n",
    "\n",
    "def _split_published_genome_data(description):\n",
    "    logger.debug(\"  splitting out primary description\")\n",
    "    try:\n",
    "        desc, pub_data = re.split(r'^#{2,3} Published Genome Data', description, maxsplit=1, flags=re.MULTILINE)\n",
    "        return desc.strip(), pub_data.strip()\n",
    "    except ValueError:\n",
    "        return description, None       \n",
    "\n",
    "def _fix_uls(soup):\n",
    "    logger.debug(\"  joining adjacent lists\")\n",
    "    previous_ul, *other_uls = soup.find_all('ul')\n",
    "    for ul in other_uls:\n",
    "        # Go back and see if the previous (non-whitespace) tag was also a ul\n",
    "        previous_tag = _previous_non_whitespace(ul)\n",
    "        if previous_tag == previous_ul:\n",
    "            if len(ul.find_all('li')) == len(ul.contents):\n",
    "                for li in ul.find_all('li'):\n",
    "                    previous_ul.append(li)\n",
    "                ul.replace_with('')\n",
    "        else:\n",
    "            previous_ul = ul\n",
    "            \n",
    "def _add_p_before_ul(soup):\n",
    "    logging.debug(\"  adding a space before each list\")\n",
    "    for ul in soup.find_all('ul'):\n",
    "        p = soup.new_tag('p')\n",
    "        ul.insert_before(p)\n",
    "    \n",
    "def get_details(link, expected_title=None):\n",
    "    logger.info(\"Building soup from %s\" % link)\n",
    "    r = requests.get(link)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    include_ajax(soup)\n",
    "    try:\n",
    "        download_panels = [panel for panel in soup.find(id='rhs').find_all(class_='panel') if panel.h3 and 'download' in panel.h3.text]\n",
    "        links = [{'text': a.text, 'url': a['href']} for a in download_panels[0].find_all('a')]\n",
    "    except IndexError:\n",
    "        links = []\n",
    "        \n",
    "    try:\n",
    "        related_panels = [panel for panel in soup.find(id='rhs').find_all(class_='panel') if panel.h5 and 'links' in panel.h5.text]\n",
    "        links += [{'text': a.text, 'url': a['href']} for a in related_panels[0].find_all('a')]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    pubmed_ids = _get_pubmed_ids(soup)\n",
    "\n",
    "    description_panel = soup.find_all(class_='panel')[0]\n",
    "    \n",
    "    _remove_bibliography(description_panel)\n",
    "    _remove_studies(description_panel)\n",
    "    _fix_uls(soup)\n",
    "    _add_p_before_ul(soup)\n",
    "    _remove_breaks_in_tables(soup)\n",
    "    _remove_spans_in_tables(soup)\n",
    "    _add_space_between_links(soup)\n",
    "    _add_break_before_tables(soup)\n",
    "    _remove_rhs(description_panel)\n",
    "    if expected_title is None:\n",
    "        title_heading = description_panel.find('h2') or description_panel.find('h3')\n",
    "        title = title_heading.text\n",
    "        _remove_title(description_panel)\n",
    "    else:\n",
    "        title = _remove_title(description_panel)\n",
    "        if title != expected_title:\n",
    "            logger.warning(\"  !!! Expected page title to be %s, got %s\" % (expected_title, title))\n",
    "    h = html2text.HTML2Text()\n",
    "    h.body_width = 0\n",
    "    description = h.handle(str(description_panel))\n",
    "    description = re.sub(r'^\\*{2,3}([^*]+)\\*{2,3}$', r'## \\1', description, flags=re.MULTILINE)\n",
    "    description = re.sub(r'^###+', r'###', description, flags=re.MULTILINE)\n",
    "    description = re.sub(r'^#', r'\\n#', description, flags=re.MULTILINE)\n",
    "    description = re.sub(r'\\n{3,}', '\\n\\n', description, flags=re.MULTILINE).strip()\n",
    "    description, published_data_description = _split_published_genome_data(description)\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'description': description,\n",
    "        'links': links,\n",
    "        'pubmed_ids': pubmed_ids,\n",
    "        'published_data_description': published_data_description\n",
    "    }\n",
    "    \n",
    "links = get_virus_links()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fail(text):\n",
    "    logging.error(\"\\x1b[31m%s\\x1b[0m\" % text)\n",
    "    \n",
    "data = []\n",
    "failures = []\n",
    "logger.setLevel(logging.INFO)\n",
    "for title, link in links:\n",
    "    try:\n",
    "        data.append(get_details(link, title))\n",
    "        time.sleep(0.5)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        fail(\"Issue getting data from %s\" % link)\n",
    "        failures.append((title, link, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "template=Template(\"\"\"\\\n",
    "---\n",
    "databases:\n",
    "{%- for database in databases %}\n",
    "- {{database}}\n",
    "{%- endfor %}\n",
    "metadata:\n",
    "  description: |\n",
    "    {{description|indent(4)}}\n",
    "  list_data: {% if list_data %}true{% else %}false{% endif %}\n",
    "  title: {{ title }}\n",
    "species:\n",
    "{%- for datum in data %}\n",
    "  {%- if datum['description'] or datum['published_data_description'] or datum['links'] or datum['pubmed_ids']%}\n",
    "  {{datum['title']}}:\n",
    "    {%- if datum['description'] %}\n",
    "    description: |\n",
    "      {{datum['description']|indent(6)}}\n",
    "    {%- endif %}\n",
    "    {%- if datum['published_data_description'] %}\n",
    "    published_data_description: |\n",
    "      {{datum['published_data_description']|indent(6)}}\n",
    "    {%- endif %}\n",
    "    {%- if datum['links'] %}\n",
    "    links:\n",
    "    {%- for link in datum['links'] %}\n",
    "    - url: {{link['url']}}\n",
    "      text: {{link['text']}}\n",
    "    {%- endfor %}\n",
    "    {%- endif %}\n",
    "    {%- if datum['pubmed_ids'] %}\n",
    "    pubmed_ids:\n",
    "    {%- for pubmed_id in datum['pubmed_ids'] %}\n",
    "    - {{pubmed_id}}\n",
    "    {%- endfor %}\n",
    "    {%- endif %}\n",
    "  {%- else %}\n",
    "  {{datum['title']}}: {}\n",
    "  {%- endif %}\n",
    "{% endfor %}\"\"\")\n",
    "\n",
    "data = list(data)\n",
    "config = template.render(data=data, databases=[''],\n",
    "                description=\"\"\"\"\"\",\n",
    "                list_data=True, title='')\n",
    "#print(config)\n",
    "print(config, file=open('page_config/viruses.yml', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "link = \"http://www.sanger.ac.uk/resources/downloads/bacteria/salmonella.html\"\n",
    "title = \"Salmonella\"\n",
    "_data = [get_details(link, title)]\n",
    "_config = template.render(data=_data, databases=['pathogen_prok_track', 'pathogen_pacbio_track'],\n",
    "                description=\"\"\"\\\n",
    "This page provides access to the genome sequence of bacteria sequenced at the Wellcome Trust Sanger Institute.\n",
    "We have sequenced a large number of bacterial genomes and make all our sequence data available. The Institute's bacterial sequencing effort concentrates on pathogenic bacteria. The data include complete, ongoing and forthcoming sequencing projects.\n",
    "You may also be interested in the following collaborative projects:\n",
    "[MetaHit](http://www.sanger.ac.uk/resources/downloads/bacteria/metahit/) - the role of the human intestinal microbiota in health and disease\n",
    "[NCTC reference collection](http://www.sanger.ac.uk/resources/downloads/bacteria/nctc/) - generation of annotated and assembled genomes for 3,000 bacteria and 500 viruses\n",
    "[BSAC projects](http://www.sanger.ac.uk/resources/downloads/bacteria/bsac/) - British Society for Antimicrobial Chemotherapy projects\"\"\",\n",
    "                list_data=True, title='Bacterial vector genomes - data download')\n",
    "print(_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "_data = [get_details(\n",
    "        \"http://www.sanger.ac.uk/resources/downloads/bacteria/streptococcus-pneumoniae.html\",\n",
    "        \"Streptococcus pneumoniae\")]\n",
    "_config = template.render(data=_data, databases=['pathogen_prok_track', 'pathogen_pacbio_track'],\n",
    "                description=\"\"\"\\\n",
    "This page provides access to the genome sequence of bacteria sequenced at the Wellcome Trust Sanger Institute.\n",
    "We have sequenced a large number of bacterial genomes and make all our sequence data available. The Institute's bacterial sequencing effort concentrates on pathogenic bacteria. The data include complete, ongoing and forthcoming sequencing projects.\n",
    "You may also be interested in the following collaborative projects:\n",
    "[MetaHit](http://www.sanger.ac.uk/resources/downloads/bacteria/metahit/) - the role of the human intestinal microbiota in health and disease\n",
    "[NCTC reference collection](http://www.sanger.ac.uk/resources/downloads/bacteria/nctc/) - generation of annotated and assembled genomes for 3,000 bacteria and 500 viruses\n",
    "[BSAC projects](http://www.sanger.ac.uk/resources/downloads/bacteria/bsac/) - British Society for Antimicrobial Chemotherapy projects\"\"\",\n",
    "                list_data=True, title='Bacterial vector genomes - data download')\n",
    "print(_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import markdown\n",
    "import logging\n",
    "\n",
    "print(config, file=open('tmp.md', 'w'))\n",
    "title, d = list(yaml.load(config)['species'].items())[0]\n",
    "f = open('site/tmp.html', 'w')\n",
    "\n",
    "markup = lambda md: markdown.markdown(md, extensions=['markdown.extensions.tables'])\n",
    "\n",
    "content = \"\"\"\\\n",
    "<html>\n",
    "<body>\n",
    "<h1>{title}</h1>\n",
    "<h2>Description</h2>\n",
    "{description}\n",
    "<h2>More details</h2>\n",
    "{more}\n",
    "</body>\"\"\".format(title=title, description=markup(d['description']),\n",
    "                  more=markup(d['published_data_description']))\n",
    "f.write(content)\n",
    "f.close()\n",
    "print(\"Done\")\n",
    "print(d['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.sanger.ac.uk/resources/downloads/bacteria/'\n",
    "soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "links = [{'url': 'http://www.sanger.ac.uk/resources/downloads/bacteria/' + a['href'], 'text': a.text} for a in soup.find_all('a') if not '/' in a['href'] and not '#' in a['href']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "tmp_dir = os.path.join(os.getcwd(), 'tmp_html')\n",
    "failures = []\n",
    "for i,url in enumerate((l['url'] for l in links)):\n",
    "    fname = os.path.basename(url)\n",
    "    path = os.path.join(tmp_dir, fname)\n",
    "    with open(path, 'w') as outfile:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code != 200:\n",
    "            failures.append((url, path))\n",
    "            continue\n",
    "        print(r.text, file=outfile)\n",
    "    time.sleep(1)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Got %s URLs with %s failures\" % (i, len(failures)))\n",
    "print(len(failures), failures)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link_counts = []\n",
    "n = len(os.listdir(tmp_dir))\n",
    "for i,path in enumerate((os.path.join(tmp_dir, fname) for fname in os.listdir(tmp_dir))):\n",
    "    soup = BeautifulSoup(open(path, 'r'), 'html.parser')\n",
    "    link_counts.append((len(soup.find(id='rhs').find_all('a')), path))\n",
    "    if i%10 == 0:\n",
    "        print(\"Done %s of %s: %s\" % (i, n, path))\n",
    "for lc in sorted(link_counts):\n",
    "    print(\"%s\\t%s\" % lc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n, path in sorted(link_counts, reverse=True):\n",
    "    url = 'http://www.sanger.ac.uk/resources/downloads/bacteria/' + os.path.basename(path)\n",
    "    print(n,url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
